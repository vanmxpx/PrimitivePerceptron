#include "stdafx.h"
#include "Crack.h"

using namespace arma;

Crack::Crack(int nr, int ep, double lr, double am)
	: result_count(1), inputs(5), outputs1(nr, fill::zeros), outputs2(nr, fill::zeros),
	w1(5, nr, fill::randn), w2(nr, nr, fill::randn), wout(nr, result_count, fill::randn),
	neur_count(nr), epoches(ep), learn_rate(lr), alpha_mom(am)
{
	fill_tests();
	mat loadw1;
	mat loadw2;
	mat loadwout;
	if (loadw1.load("crack_wghts_layer1.txt") &&
		loadw2.load("crack_wghts_layer2.txt") &&
		loadwout.load("crack_wghts_out.txt"))
	{
		w1 = loadw1;
 		w2 = loadw2;
		wout = loadwout;
	}
}

//logariphm
//sin
double Crack::normalizeAnswer(double answer)
{
	return 1 / (1 + exp(-answer));//log(answer + sqrt(answer*answer + 1));//
}
double Crack::gradFunc(double answer)
{
	return (1 - answer)*answer; //1 / sqrt(answer*answer + 1);//
}
double Crack::normalaizeInput(double answer)
{
	return (1 - answer)*answer; //(answer - 0) / sqrt(answer*answer + 1);//
}

vec Crack::analize(vec number)
{
	vec result(result_count, fill::zeros);
	inputs = number;
	double tmp;
	for (int row = 0; row < neur_count; row++)
	{
		//inputs sinapses
		tmp = 0;
		for (int col = 0; col < 5; col++)//(3 dendrids in 2000 neurons)
			tmp += inputs.at(col) * w1.at(col, row);
		outputs1.at(row) = normalizeAnswer(tmp);
	}
	for (int row = 0; row < neur_count; row++)
	{
		//inputs sinapses
		tmp = 0;
		for (int col = 0; col < neur_count; col++)////(2000 dendrids in 2000 neurons)
			tmp += outputs1.at(col) * w2.at(col, row);
		outputs2.at(row) = normalizeAnswer(tmp);
	}
	//hidden sinapses
	for (int row = 0; row < result_count; row++)
	{
		tmp = 0;
		for (int col = 0; col < neur_count; col++) //(2000 dendrids in 27 neurons)
			tmp += outputs2.at(col) * wout.at(col, row);
		result.at(row) = normalizeAnswer(tmp);
	}
	return result;
}

void Crack::test()
{
	vec res(5);
	for (int k = 0; k < 5; k++)
	{
		auto asd = instest.row(k);
		Col<double> temp(5);
		for (int s = 0; s < 5; s++) {
			temp.at(s) = asd[s];
		}

		res.at(k) = analize(temp).at(0);
	}
	cout << "test:" << endl << res;
}
void Crack::learn()
{
	mat delta(neur_count, 3); //(2000, 2000, result_count)
	mat w1last(5, neur_count, fill::zeros);
	mat w2last(neur_count, neur_count, fill::zeros);
	mat woutlast(neur_count, result_count, fill::zeros);

	vec err(result_count, fill::ones);
	vec res(result_count);
	vec expected(result_count);
	vec lastres(30, fill::zeros);
	int true_results = 0;
	for (int i = 0; i < epoches; i++) //&& err != 0
	{

		cout << "------------------" << endl;

		//test inpits count = 30
		for (int k = 0; k < 28; k++)
		{
			delta.fill(fill::zeros);
			//analize
			auto asd = ins.row(k);
			Col<double> temp(5);
			for (int s = 0; s < 5; s++) {
				temp.at(s) = asd[s];
			}

			res = analize(temp);
			double res2 = res.at(0);
			double expected2;
			//auto asd2 = valuesP.row(k);// change for crack
			//for (int s = 0; s < result_count; s++) {
			//	expected.at(s) = valuesP.at(k);//asd2[s];
			//}
			//Calc errors

			expected2 = valuesP.at(k);
			double err2 = (expected2 - res2) * (expected2 - res2);//pow(expected2 - res2, 2);
			double deltaerr=0;
			for (int e = 0; e < result_count; e++)
				deltaerr += err2;
			deltaerr /= result_count;
			//if (deltaerr < 0.00001) true_results++; if (true_results > 50) save_weights();
			double increased = deltaerr - lastres.at(k);
#ifndef DEBUG
			cout << increased << ": " << deltaerr << ": " << res.at(0) << endl;
			//cout << res  << endl;
#endif

			lastres.at(k) = deltaerr;
			//Error spread
			//Calc deltaes
			for (int row = 0; row < result_count; row++) //OUT delta func
				//delta.at(row, 2) = (expected.at(row) - res.at(row)) * gradFunc(res.at(row));
				delta.at(row, 2) = (expected2 - res2) * gradFunc(res2);
			double tmp;
			for (int row = 0; row < neur_count; row++)
			{
				tmp = 0;
				for (int col = 0; col < result_count; col++)//summ of weights, what binded to output neuron
					tmp += wout.at(row, col) * delta.at(col, 2);

				delta.at(row, 1) = gradFunc(outputs2.at(row)) * tmp;
			}

			//Uncomment to implement more layers
			for (int row = 0; row < neur_count; row++) //for input neirons
			{
				tmp = 0;
				for (int col = 0; col < neur_count; col++)//summ of weights, what binded to 2 layer
					tmp += w2.at(row, col) * delta.at(col, 1);

				delta.at(row, 0) = gradFunc(outputs1.at(row)) * tmp;
			}

			//Calc grad
			for (int row = 0; row < neur_count; row++)
			{
				for (int col = 0; col < neur_count; col++)
				{
					if (row < 5)
					{
						double grad = inputs.at(row) * delta.at(col, 0);
						w1.at(row, col) += learn_rate * grad + alpha_mom * w1last.at(row, col);
					}

					// back propagation for outputs neurons
					double grad2 = outputs1.at(row) * delta(col, 1);
					w2.at(row, col) += learn_rate * grad2 + alpha_mom * w2last.at(row, col);

					if (col < result_count) // back propagation for outputs neurons 
					{
						double gradout = outputs2.at(row) * delta(col, 2);
						wout.at(row, col) += learn_rate * gradout + alpha_mom * woutlast.at(row, col);
					}
				}
			}


			w1last = w1;
			w2last = w2;
			woutlast = wout;
		}
	}
	test();
	system("pause");
}

bool Crack::save_weights() const {
	return w1.save("crack_wghts_layer1.txt", raw_ascii) &&
		w2.save("crack_wghts_layer2.txt", raw_ascii) &&
		wout.save("crack_wghts_out.txt", raw_ascii);
}

void Crack::fill_tests()
{
	instest = { { 0.03, 0.3, 0.2, 1, 10 },
		{0.06, 0.2, 0.3, 1, 10 },	
	{ 0.12, 0.3, 0.2, 10, 1 },
	{ 0.23, 0.3, 0.2, 1, 10 },
	{ 0.5, 0.2, 0.3, 10, 1 } };

	ins = {	//{ 0.05, 0.2, 0.3, 10, 1 },
		{ 0.075, 0.2, 0.3, 10, 1 },
		{ 0.1, 0.2, 0.3, 10, 1 },
		{ 0.125, 0.2, 0.3, 10, 1 },
		{ 0.15, 0.2, 0.3, 10, 1 },
		{ 0.2, 0.2, 0.3, 10, 1 },
		{ 0.25, 0.2, 0.3, 10, 1 },
		{ 0.3, 0.2, 0.3, 10, 1 },
		{ 0.4, 0.2, 0.3, 10, 1 },
		//{ 0.05, 0.3, 0.2, 1, 10 }, //remove it for Crack
			{0.075, 0.3, 0.2, 1, 10},
		{ 0.1, 0.3, 0.2, 1, 10 },
		{ 0.125, 0.3, 0.2, 1, 10 },
	{ 0.15, 0.3, 0.2 , 1, 10 },
	{ 0.2, 0.3, 0.2, 1, 10 },
	{ 0.25, 0.3, 0.2, 1, 10 },
	{ 0.3, 0.3, 0.2, 1, 10 },
	{ 0.4, 0.3, 0.2, 1, 10 },
	{ 0.1, 0.2, 0.3, 1, 10 },
	{ 0.15, 0.2, 0.3, 1, 10 },
	{ 0.2, 0.2, 0.3, 1, 10 },
	{ 0.25, 0.2, 0.3, 1, 10 },
	{ 0.3, 0.2, 0.3, 1, 10 },
	{ 0.4, 0.2, 0.3, 1, 10 },

	{ 0.125, 0.2, 0.3, 30, 1 },
	{ 0.15, 0.2, 0.3, 30, 1 },
	{ 0.2, 0.2, 0.3, 30, 1 },
	{ 0.25, 0.2, 0.3, 30, 1 },
	{ 0.3, 0.2, 0.3, 30, 1 },
	{ 0.4, 0.2, 0.3, 30, 1 } };

	
	valuesP = { 				/*0.007,*/  0.01,    0.016,   0.024,   0.0315,  0.0497,  0.068,   0.084,   0.12, //0.2,0.3, 10, 1
		/*0.0085,*/ 0.01212, 0.02439, 0.03317, 0.04971, 0.07382, 0.10874, 0.1369, 0.20389, //0.3,0.2, 1, 10
		0.02239, 0.04807, 0.07349, 0.10134, 0.13081, 0.19091, //0.2,0.3, 1, 10

		0.02, 0.037, 0.04, 0.054, 0.068, 0.089 };//0.2,0.3, 30, 1

	valuesCrack = { /*{ 0.008453524845833,
		0.0848380480238123,
		0.312166645692774,
		0.516193253935563,
		0.690435806822004,
		0.837975725366429,
		0.962775978094076,
		1.06833714733145,
		1.1576192364141,
		1.23311361398321,
		1.29691350876918,
		1.3507739762765,
		1.39616369943036,
		1.43430955097424,
		1.46623429083954,
		1.49278791741774,
		1.51467338239257,
		1.53246744371968,
		1.54663739199852,
		1.55755429233977,
		1.56550327280812,
		1.57069128047801,
		1.57325262472705 },*/
		{ 0.02263, 1.56319371796451,
		1.56319304792539,
		1.56067936249605,
		1.55557063896382,
		1.54771667256935,
		1.53689515107146,
		1.52280573176461,
		1.50506187285705,
		1.48318007478949,
		1.45656607745671,
		1.42449743653129,
		1.38610177153999,
		1.34032985526896,
		1.28592262905261,
		1.22137122496981,
		1.14486916682147,
		1.05425592067649,
		0.946950146940035,
		0.819868798113614,
		0.669336586223703,
		0.491121600373299,
		0.281669048767794,
		0.0465839091231731,
		-0.157587950484647,
		-0.0860996642674238,
		0.555732940297477 },
		{ 0.03317, 1.55890119935002,
		1.55890036184866,
		1.55640820865852,
		1.55133790498423,
		1.54353488880975,
		1.53277284204781,
		1.51874766290727,
		1.50106918515015,
		1.47925029169252,
		1.45269295735179,
		1.42067062798046,
		1.38230620511036,
		1.336544770248,
		1.28212007934021,
		1.2175138258504,
		1.14090671895776,
		1.05012035715578,
		0.942547934470013,
		0.815069220776082,
		0.663952010343367,
		0.484866951206427,
		0.27405841326301,
		0.0366655829203988,
		-0.171593136875136,
		-0.104879533086548,
		0.547312504437236, },
	{ 0.04971,1.55273648081264,
		1.55273515232373,
		1.55027207049377,
		1.54525051692298,
		1.5375065988546,
		1.52680500302321,
		1.51283274293474,
		1.49519063335594,
		1.4733821243154,
		1.44679900913495,
		1.41470338548805,
		1.37620509804947,
		1.33023373725145,
		1.27550413561729,
		1.21047422621114,
		1.13329410883094,
		1.04174500251139,
		0.933165639073733,
		0.804360548638144,
		0.651489454925335,
		0.470054184617262,
		0.255994599376832,
		0.013854789534632,
		-0.201346888773949,
		-0.140262618768094,
		0.533675731383986 },
	{ 0.07782,1.54075731573054,
		1.54075534156367,
		1.538351196949,
		1.53343418316221,
		1.52582765772701,
		1.51528448332031,
		1.50148046500565,
		1.48400549113943,
		1.46235198629571,
		1.43590015790066,
		1.40389936869305,
		1.36544479450104,
		1.31944833771339,
		1.26460257857367,
		1.19933638363669,
		1.1217606408853,
		1.02960222237698,
		0.920122779914596,
		0.790014792402863,
		0.635267609861923,
		0.451095272611783,
		0.232853964643018,
		-0.0162628827314069,
		-0.243704161405116,
		-0.197246958593521,
		0.506787200404906 },
	{ 0.10874,1.5287406996336,
		1.52873808591555,
		1.5263925608828,
		1.52157957071106,
		1.51410984109217,
		1.5037245216019,
		1.49008831316601,
		1.47278027525272,
		1.45128189681226,
		1.42496187928157,
		1.3930569160202,
		1.35464755711655,
		1.30862802319461,
		1.25366858616935,
		1.18816888181157,
		1.11020022953752,
		1.01743444933595,
		0.907054778675289,
		0.775639217293366,
		0.619002555607809,
		0.432064017639887,
		0.209581121164465,
		-0.0466597513987656,
		-0.286821310729838,
		-0.256441528547307,
		0.477050832374471 },
	{ 0.14121,1.51637456188542,
			1.51637136139551,
			1.51408635488645,
			1.50938223800405,
			1.50205827665368,
			1.4918451703175,
			1.4783978958107,
			1.46128620999856,
			1.43998237181714,
			1.41384549916508,
			1.38210179773734,
			1.34381968020196,
			1.29787853154235,
			1.24292957096194,
			1.17734691253761,
			1.09916648822886,
			1.00600967743462,
			0.894986183085722,
			0.762564297354334,
			0.604388095199596,
			0.415082206742252,
			0.188787672774954,
			-0.074230290637776,
			-0.327384085485417,
			-0.315557229274296,
			0.444034057898389 },
	{ 0.20659,1.49187892312065,
			1.49187473562037,
			1.48971031505727,
			1.48522796707215,
			1.47820923350804,
			1.46836930606189,
			1.45534942476315,
			1.4387069082019,
			1.4179023340573,
			1.3922832268296,
			1.36106340375319,
			1.32329686672354,
			1.27784479319617,
			1.22333375583926,
			1.15810276172161,
			1.0801359585944,
			0.986976553407815,
			0.875614341327249,
			0.742330693514688,
			0.582467550903227,
			0.390108504642269,
			0.158206230531344,
			-0.116130429695499,
			-0.394022096194715,
			-0.424427303654504,
			0.371906882297459 },
		//v1 02
	{ 0.02139,1.94503036300448,
			1.9450292757804,
			1.94004560224907,
			1.92992568376372,
			1.914385419483,
			1.89300498283634,
			1.86522112314052,
			1.83031708144006,
			1.7874102420234,
			1.73543782871186,
			1.67314130374111,
			1.5990507482718,
			1.51147159746201,
			1.40847807407594,
			1.28792147621775,
			1.14746980167283,
			0.984716150629793,
			0.797452118910054,
			0.584374983585175,
			0.346997833712629,
			0.0948837830322464,
			-0.140582405044002,
			-0.274584310560756,
			-0.122794332262696,
			0.511429477824283,
			0.774952820824308 },
	{ 0.04807,1.92422160005286,
			1.92421940590866,
			1.91937021652603,
			1.90949711010488,
			1.89429563609818,
			1.87332804083361,
			1.84601505319375,
			1.81162522714346,
			1.76926191787612,
			1.71784813851729,
			1.65610986809974,
			1.58255896618652,
			1.49547788601799,
			1.39291024488156,
			1.27266489229776,
			1.13234885617285,
			0.969463929283733,
			0.781656359803228,
			0.567372423502271,
			0.327657252052503,
			0.0711831067884449,
			-0.172184756349261,
			-0.31915661464402,
			-0.182189194142849,
			0.45841142285498,
			0.782664034047129 },
	{ 0.07349,1.90452049377089,
				1.90451733164808,
				1.8997949887041,
				1.89015641379649,
				1.87527979849063,
				1.85471236988882,
				1.82786172107255,
				1.79398464513271,
				1.75217350660144,
				1.70134033689762,
				1.64019913905374,
				1.56724743461083,
				1.480749064208,
				1.37872201750689,
				1.25893843024477,
				1.11895107258785,
				0.956178542044862,
				0.76813213565041,
				0.553021331029956,
				0.311442016824117,
				0.0511893856646629,
				-0.199436636315168,
				-0.358967802503124,
				-0.237591403835548,
				0.405979443383495,
				0.788686964635832 },
	{ 0.10134,1.88385620998684,
				1.88385211753763,
				1.87926235275285,
				1.86987080448989,
				1.85533952810223,
				1.83520212009568,
				1.80885460862919,
				1.77554379348674,
				1.7343530238957,
				1.68418553639451,
				1.62374574586708,
				1.55151938927959,
				1.46575433613311,
				1.36444553214271,
				1.24533067269462,
				1.10590981481168,
				0.943518470789082,
				0.755530300198559,
				0.539909139459167,
				0.296776622796473,
				0.0329792308233842,
				-0.224948305529789,
				-0.397842903001352,
				-0.29439155217933,
				0.348863485544046,
				0.793439974025409 },
	{ 0.13081,1.86291820749764,
				1.86291326631789,
				1.85845740900487,
				1.84931748825154,
				1.83514150834287,
				1.81545161717628,
				1.78963459443179,
				1.75692973696545,
				1.71641408373213,
				1.66698503647416,
				1.60734067394225,
				1.53595851981163,
				1.45107437057163,
				1.35066432519237,
				1.23243605205369,
				1.0938413675909,
				0.932136982452855,
				0.744562578073363,
				0.528838081023836,
				0.284605114397185,
				0.0177255529919194,
				-0.247173886063193,
				-0.433681050077591,
				-0.349965048259102,
				0.289110077362981,
				0.796366088438823 },
	{ 0.19091,1.82335771306946,
				1.82335143196165,
				1.81914743435814,
				1.8104867322897,
				1.79699719896291,
				1.77818595695106,
				1.75342923326098,
				1.7219595224838,
				1.68284991444297,
				1.63499551785573,
				1.57709209473652,
				1.5076123945036,
				1.4247813857844,
				1.32655288971119,
				1.210592565638,
				1.07427719114432,
				0.914732130332973,
				0.728963222701362,
				0.514250612292597,
				0.269345164983685,
				-0.00178459407765958,
				-0.278388259632299,
				-0.49034152864298,
				-0.447772402526952,
				0.172253865107745,
				0.795810859298339 }};


}
